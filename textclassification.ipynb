{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhQ7uFJUhWbeaqLZPwnqTf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahekbagde/ML/blob/main/textclassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CTA-14 Mahek Bagde"
      ],
      "metadata": {
        "id": "X5ScaxsYK5cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Text classification using Word2Vec Python"
      ],
      "metadata": {
        "id": "vBYS1q91K2wJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlU5xjSC7tki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90ff5c01-2294-408c-f087-f05c49ef0aaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn gensim nltk\n",
        "#gensim for word embeddings and topic modeling, and nltk (the Natural Language Toolkit for text processing and analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Tzvd-BBMLRaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/sample_data/movie_review.csv')\n",
        "#select the column named 'text', the column named 'tag' from data.\n",
        "#test_size indicating that 20% of the data will be reserved for testing, remaining 80% will be used for training.\n",
        "#random_seed - running the code multiple times will yield the same results in terms of the data split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['tag'], test_size = 0.2, random_state = 42)\n"
      ],
      "metadata": {
        "id": "hjdvEFxDLfci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "#Stopwords are common words (like \"the\", \"is\", \"and\", etc.) that are often removed from\n",
        "#  text data because they don't usually convey significant meaning.\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "#String operations but here used to access the punctuation characters.\n",
        "import string\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2Y6tn43Mw9v",
        "outputId": "bd4d3f91-c6eb-4284-98ce-e998df94c611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loads the set of English stopwords from the NLTK corpus and stores them in a variable called stop_words.\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess(text):\n",
        "  #Convert the text to lowercase using lower() method.\n",
        "  text = text.lower()\n",
        "  #Remove punctuation from the text using list comprehension and join() method.\n",
        "  text = ''.join([word for word in text if word not in string.punctuation])\n",
        "  #Tokenize the text into words using word_tokenize() function.\n",
        "  tokens = word_tokenize(text)\n",
        "  #Remove stopwords from the tokenized text using list comprehension and the stopwords set (stop_words).\n",
        "  tokens = [word for word in tokens if word not in stop_words]\n",
        "  #Join the filtered tokens back into a single string using join() method.\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "X_train = X_train.apply(preprocess)\n",
        "X_test = X_test.apply(preprocess)\n"
      ],
      "metadata": {
        "id": "7JjwhGnaWbas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtFKOlR3XFaf",
        "outputId": "4241f35c-62bb-43a6-b628-99009ed1f1fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48063    patch never seemed understand possibility mayb...\n",
              "7748     film stars matt damon hunting mathematical reb...\n",
              "39824                      square one bizarre case casting\n",
              "49957                             become walking wardrobes\n",
              "33990    hong kong film tempo changes heightening emoti...\n",
              "                               ...                        \n",
              "62570    laughable moments include luc going strip club...\n",
              "38158    even aspect film fails throwing convenient rid...\n",
              "860      schreber contacted john reasons continually re...\n",
              "15795    sure excellent bound usual suspects spring imm...\n",
              "56422    dismal thirdrate farrelly brothers rip attempt...\n",
              "Name: text, Length: 51776, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgsuRz57XII6",
        "outputId": "2beb355b-4e4d-44fb-861f-2c8b08fbfe00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58154                            like dream without appeal\n",
              "33401    stateoftheart special effects never carpenter ...\n",
              "44182    action films action sequences conventional att...\n",
              "46480    number reasons including fact experienced film...\n",
              "41584    julie james jennifer love hewitt ray bronson f...\n",
              "                               ...                        \n",
              "4456                                 exactly fifth element\n",
              "27023    motion picture adapted elmore leonards novel r...\n",
              "17103    scene ends tragedy clear theres kind powerful ...\n",
              "63177         much horror everyone standing within earshot\n",
              "49820    bills youngest daughter susan claire forlani f...\n",
              "Name: text, Length: 12944, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the Word2Vec model\n",
        "\n",
        "Train a Word2Vec model on the preprocessed training data using Gensim package."
      ],
      "metadata": {
        "id": "Mh6Op05uXVQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Word2Vec class in Gensim is used to train word embeddings using the Word2Vec algorithm.\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "sentences = [sentence.split() for sentence in X_train]\n",
        "w2v_model = Word2Vec(sentences,  window=5, min_count=5, workers=4)#workers - no of cpus"
      ],
      "metadata": {
        "id": "lG2kwqziXOts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize the text data"
      ],
      "metadata": {
        "id": "JsYSEZ6agGy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize(sentence):\n",
        "   #splits the input sentence into individual words and stores them in a list named words.\n",
        "   words = sentence.split()\n",
        "   #For each word, it checks if the word exists in the vocabulary of the Word2Vec model\n",
        "   # adds it to the words_vecs list.\n",
        "   word_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
        "   if len(word_vecs) == 0:\n",
        "       return np.zeros(100)\n",
        "   words_vecs =  np.array(word_vecs)\n",
        "   return words_vecs.mean(axis = 0)\n",
        "\n",
        "X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
        "X_test = np.array([vectorize(sentence) for sentence in X_test])"
      ],
      "metadata": {
        "id": "rt5cA7eMgF5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Classification model"
      ],
      "metadata": {
        "id": "AAbyJdAzvyHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "diApdH4gvv64",
        "outputId": "241e96d5-ab04-46cd-9891-f0a0d64b4401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model"
      ],
      "metadata": {
        "id": "hbpOwry6wAMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "y_pred = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "ALU-dcM0v_UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as metrics\n",
        "Accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "Precision = metrics.precision_score(y_test, y_pred, pos_label='pos')\n",
        "Sensitivity_recall = metrics.recall_score(y_test, y_pred,pos_label='pos')\n",
        "Specificity = metrics.recall_score(y_test, y_pred, pos_label='pos')\n",
        "F1_score = metrics.f1_score(y_test, y_pred, pos_label='pos')\n",
        "print({\"Accuracy\":Accuracy,\"Precision\":Precision,\"Sensitivity_recall\":Sensitivity_recall,\"Specificity\":Specificity,\"F1_score\":F1_score})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_qG7X6Qwkly",
        "outputId": "66bced68-c141-436c-b6b2-563b435ff4f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Accuracy': 0.5197775030902348, 'Precision': 0.5159560203807991, 'Sensitivity_recall': 0.8781378366042902, 'Specificity': 0.8781378366042902, 'F1_score': 0.65}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing pos_label from 'pos' to 'neg' essentially swaps the positive and negative classes for the computation\n",
        "#of precision and recall metrics. This can be useful in scenarios where you want to focus on the performance of the classifier for\n",
        "#a specific class (positive or negative).\n",
        "\n",
        "import sklearn.metrics as metrics\n",
        "Accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "Precision = metrics.precision_score(y_test, y_pred, pos_label='neg')\n",
        "Sensitivity_recall = metrics.recall_score(y_test, y_pred,pos_label='neg')\n",
        "Specificity = metrics.recall_score(y_test, y_pred, pos_label='neg')\n",
        "F1_score = metrics.f1_score(y_test, y_pred, pos_label='neg')\n",
        "print({\"Accuracy\":Accuracy,\"Precision\":Precision,\"Sensitivity_recall\":Sensitivity_recall,\"Specificity\":Specificity,\"F1_score\":F1_score})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9nNfgIL5N5S",
        "outputId": "dcf40537-d2c6-47fd-c06b-ec57c7a15dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Accuracy': 0.5197775030902348, 'Precision': 0.5441092771770063, 'Sensitivity_recall': 0.1500549364307016, 'Specificity': 0.1500549364307016, 'F1_score': 0.23523622047244094}\n"
          ]
        }
      ]
    }
  ]
}